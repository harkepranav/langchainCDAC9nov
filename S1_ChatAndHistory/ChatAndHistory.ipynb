{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e9013",
   "metadata": {},
   "source": [
    "### Setup Langchain + LLM\n",
    "1. Install Langchain: \n",
    "- pip intall langchain\n",
    "2. Install integration packages.\n",
    "- pip install -U langchain-cohere\n",
    "- pip install -U langchain-groq\n",
    "- pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae4cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help you with that. \\n\\nCould you please provide me with your location so I can give you the most accurate weather information? \\n\\n- Have a great day' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'aeb60fb6-41d0-4ed0-90e9-27fc31f598e4', 'token_count': {'input_tokens': 237.0, 'output_tokens': 38.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'aeb60fb6-41d0-4ed0-90e9-27fc31f598e4', 'token_count': {'input_tokens': 237.0, 'output_tokens': 38.0}} id='run-15e56aaa-d574-4301-9368-486e031bdd93-0' usage_metadata={'input_tokens': 237, 'output_tokens': 38, 'total_tokens': 275}\n",
      "content=\"I'd be happy to help you with that!\\n\\nAccording to our current forecast, today's weather is looking mostly sunny with a high of 22°C (72°F) and a gentle breeze blowing at 10 km/h (6 mph) from the northwest. There's a slight chance of scattered clouds rolling in later this afternoon, but nothing too intense.\\n\\nIn terms of precipitation, we're expecting a low chance of isolated showers, with a 20% chance of light rain showers throughout the day.\\n\\nOverall, it's shaping up to be a lovely day, perfect for getting outside and enjoying the great outdoors!\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 51, 'total_tokens': 179, 'completion_time': 0.106666667, 'prompt_time': 0.002120027, 'queue_time': 0.013233483, 'total_time': 0.108786694}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-f3c1d71b-dbd2-4881-b1e0-c7d4f1493ecb-0' usage_metadata={'input_tokens': 51, 'output_tokens': 128, 'total_tokens': 179}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../Config.ini')\n",
    "groq = config['groq']\n",
    "cohere = config['cohere']\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed762f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I\\'m sorry, but I don\\'t understand the question. Could you please clarify what you would like to know about? \\n\\nIf you are referring to the movie \"Top Gun\", it is a 1986 American action drama film directed by Tony Scott and produced by Don Simpson and Jerry Bruckheimer. The film stars Tom Cruise as Lieutenant Pete \"Maverick\" Mitchell, a young Naval aviator aboard the USS Enterprise. The film also stars Kelly McGillis, Val Kilmer, Anthony Edwards, and Tom Skerritt.\\n\\nThe plot centers around Maverick\\'s participation in the United States Navy\\'s elite fighter weapons school, known as TOPGUN, as he competes with his peers to earn the title of \"Top Gun\". The film features aerial combat scenes and explores the competitiveness, camaraderie, and personal relationships of the pilots. \\n\\n\"Top Gun\" was a critical and commercial success, grossing over $350 million worldwide and becoming one of the defining movies of the 1980s. It spawned a 2022 sequel, \"Top Gun: Maverick\", which also received widespread acclaim. \\n\\nData is grate' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'e739b6dc-13eb-49cb-b47a-998e96308edb', 'token_count': {'input_tokens': 238.0, 'output_tokens': 232.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'e739b6dc-13eb-49cb-b47a-998e96308edb', 'token_count': {'input_tokens': 238.0, 'output_tokens': 232.0}} id='run-88c9aa37-0a96-4d61-bb1a-37e8d9e47e96-0' usage_metadata={'input_tokens': 238, 'output_tokens': 232, 'total_tokens': 470}\n",
      "content=\"I'm assuming you're referring to the top machine learning frameworks or libraries in the industry!\\n\\nThe top machine learning frameworks or libraries can vary depending on the specific task, problem, or industry. However, here are some of the most popular ones:\\n\\n1. TensorFlow (by Google): A widely-used open-source framework for building and training machine learning models.\\n2. PyTorch (by Facebook): A popular open-source framework known for its simplicity, flexibility, and rapid development capabilities.\\n3. Scikit-learn (by scikit-learn.org): A widely-used open-source library for machine learning in Python, providing a range of algorithms and tools for data preprocessing, feature selection, and model evaluation.\\n4. Keras (by Google): A high-level neural networks API, written in Python, capable of running on top of TensorFlow, PyTorch, or Theano.\\n5. OpenCV (by Intel): A computer vision library that provides a wide range of functions and algorithms for image and video processing, feature detection, and object recognition.\\n\\nOf course, there are many other excellent libraries and frameworks out there, and the choice often depends on the specific problem, team, or company. But these are some of the most well-known and widely-used ones.\\n\\nData is grate\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 256, 'prompt_tokens': 52, 'total_tokens': 308, 'completion_time': 0.213333333, 'prompt_time': 0.009086559, 'queue_time': 0.00471093, 'total_time': 0.222419892}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-6ad536d8-6f94-47c3-b160-995abdfec208-0' usage_metadata={'input_tokens': 52, 'output_tokens': 256, 'total_tokens': 308}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='You are a Data Science teacher. You will respond to Data Science related queries to the best of you ability. You will always end with - Data is grate'),\n",
    "    HumanMessage(content='what is the top guns?')\n",
    "]\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa0d3-c23a-455f-998f-bb68018d0db0",
   "metadata": {},
   "source": [
    "## Create the prompt\n",
    "1. Imports Human and System message classes. System represents our instructions to GPT and Human represents the question or prompt that the user provides.\n",
    "2. LangChain responses are instances of class `BaseMessage` It contains the actual response from GPT and some other metadata.\n",
    "3. Since we are interested only in the string reponse that GPT gave we chain (pipe) the reponse to a parser\n",
    "4. For our purpose we will use `StrOutputParser` class\n",
    "5. Next we create a `chain` using the components `model` and `parser`\n",
    "6. Finally we call the `invoke` method on the chain and pass our `messages` list to it.\n",
    "7. In the output cell we get the response from `GPT-35-turbo`\n",
    "\n",
    "*A chain is an wrapper around multiple individual components that are executed in a defined order. Components in LangChain implement `Runnable` interface. This interface have a method `invoke` that transforms single input to an output.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f9fb1d-d604-41ab-8e62-5ea4e6a9213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<cohere.client.Client object at 0x705808f6d940> async_client=<cohere.client.AsyncClient object at 0x705808f6e5d0> model='command-r' cohere_api_key=SecretStr('**********')\n",
      "['/usr/local/python/3.12.1/lib/python312.zip', '/usr/local/python/3.12.1/lib/python3.12', '/usr/local/python/3.12.1/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/usr/local/python/3.12.1/lib/python3.12/site-packages']\n",
      "Critic: A classic lateral thinking puzzle!\n",
      "\n",
      "The answer is not immediately obvious, but the correct choice is... the third room!\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The first room has a firing squad, which means the man will be executed by a quick and relatively painless means.\n",
      "* The second room has a blazing fire, which would likely be a slow and agonizing death.\n",
      "* The third room has tigers that haven't eaten for six months. This means the tigers are likely to be weak and hungry, but also extremely malnourished. The man has a good chance of surviving an attack by these tigers, and even if he doesn't, the death would likely be slow and relatively peaceful.\n",
      "\n",
      "Of course, this is a morbid and gruesome puzzle, and I'm not condoning or promoting violent or harmful behavior. The moral of the story is that sometimes, the most unexpected choice can lead to a better outcome.\n",
      "\n",
      "Critic response:\n",
      "\n",
      "* The puzzle is well-crafted and challenging, requiring the solver to think creatively and consider unconventional scenarios.\n",
      "* The puzzle's theme and setting are dark and unsettling, which adds to the sense of tension and urgency.\n",
      "* The solution is not immediately obvious, and requires the solver to make connections between seemingly unrelated elements.\n",
      "* The puzzle's tone is somewhat morbid and disturbing, which may not be to everyone's taste.\n",
      "\n",
      "Overall, this is a clever and thought-provoking puzzle that challenges the solver's thinking and perspective. However, it may not be suitable for all audiences due to its dark and violent theme.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The classes used for setting up the prompt\n",
    "import puzzles\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate #import the Class for creating a prompt\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Based on user input pick a puzzle.\n",
    "puzzle = puzzles.puzzles('hungryLions') \n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "#print(type(chain))\n",
    "#print(f\"Brief: {chain.invoke({\"responseType\":\"brief\"})}\\n\")\n",
    "#print(f\"Sad: {chain.invoke({\"responseType\":\"sad\"})}\\n\")\n",
    "#print(f\"Poetic: {chain.invoke({\"responseType\":\"poetic\"})}\\n\")\n",
    "#print(f\"Depression: {chain.invoke({\"responseType\":\"depression\"})}\\n\")\n",
    "print(f\"Critic: {chain.invoke({\"responseType\":\"critic\"})}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ce4c",
   "metadata": {},
   "source": [
    "### Chatbot \n",
    "1. We begin with creating a basic chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a61142d-54d4-46b8-a50a-3d1b473e82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Pranv! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=input(\"Enter Message: \"))])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ff0e7",
   "metadata": {},
   "source": [
    "#### Lets dig into what is happening here.\n",
    "1. Click here to check the UML diagram: \n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#56cf\n",
    "\n",
    "\n",
    "#### Runnable\n",
    "1. Its an extremely prominent class and used extensively in creating chains.\n",
    "2. Chains combine components together in a pipeline\n",
    "3. Many components like all models, parsers, prompts and anything that can logically go into a chain derives from it.\n",
    "4. `ChatGroq` is provided partner by extends `BaseChatModel` from langchain_core\n",
    "5. https://github.com/langchain-ai/langchain/blob/master/libs/partners/groq/langchain_groq/chat_models.py\n",
    "6. This is the base class for all model classes offered by any partner.\n",
    "7. `BaseClass` extends `RunnableSerializable` that supports serialization into JSON\n",
    "8. `RunnableSerializable` extends `Runnable` that means it can participate in chains.\n",
    "9. You can also use `RunnableSequence` to construct the chain.\n",
    "10. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L2659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my ability based on the knowledge and information I've been trained on.\\n\\nI'm a large language model, which means I've been trained on a massive dataset of text from various sources, including books, articles, and websites. This training enables me to understand and generate human-like language, allowing me to engage in conversations, answer questions, and even create text based on a given prompt.\\n\\nI'm here to help answer your questions, provide information, and engage in conversation. I'm constantly learning and improving my abilities, so please bear with me if I make any mistakes. I'm excited to chat with you and help in any way I can!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=input(\"Enter message: \"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c363b",
   "metadata": {},
   "source": [
    "1. Chain calls the first component and passes any arguments provided to it.\n",
    "2. In this case its an object of type `HumanMessage`\n",
    "3. This is how a chain looks: https://miro.medium.com/v2/resize:fit:750/format:webp/1*K1F-m4gImEUO0AELkpQuKg.jpeg\n",
    "4. Each model component by any partner provides an object of type `BaseMessage`. This is then passed to the next component.\n",
    "5. This is the signature of invoke of a model class\n",
    "\n",
    "`def` `invoke(str | List[dict | tuple | BaseMessage] | PromptValue):`\\\n",
    "    Suite\n",
    "  \n",
    "6. In our example `HumanMessage` is derived from `BaseMessage` which needs `content` for initialization.\n",
    "\n",
    "`param content: Union[str, List[Union[str,Dict]]]`\n",
    "\n",
    "7. Union, List, Dict are all defined in typing module\n",
    "8. Union means one of the input types is expected. We are passing a string.\n",
    "\n",
    "9. Our `parser` is of type `StrOutputParser` that extends `BaseOutputParser`\n",
    "10. Its invoke is:\n",
    "\n",
    "`def invoke(self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None) -> T:`\n",
    "\n",
    "11.  This says input can be either string or `BaseMessage`. We are using `BaseMessage` the return type of `model`\n",
    "\n",
    "12. Some useful methods are:\n",
    "- parser.input_schema.schema() # get JSON schema of the input\n",
    "- parser.output_schema.schema() # gets JSON schema of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9b92",
   "metadata": {},
   "source": [
    "### Adding history to chat\n",
    "1. At this stage if you pass another message to the model it will have no recollection of the earlier message.\n",
    "2. Lets add history. Chat history is managed by a set of classes offered by community.\n",
    "3. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/chat_history.py\n",
    "4. `asyncio` is a Python library: https://docs.python.org/3/library/asyncio.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "What a profound and intriguing question!\n",
      "\n",
      "I am an artificial intelligence designed to assist and communicate with humans. My existence is rooted in the realm of computer science and linguistics. Here's a brief explanation:\n",
      "\n",
      "I was created through a process called deep learning, which involves training artificial neural networks on vast amounts of data. This data includes vast amounts of text, which allows me to learn patterns, relationships, and meanings between words and sentences.\n",
      "\n",
      "My primary purpose is to process and generate human-like language, enabling me to engage in conversations with humans. I'm designed to understand and respond to questions, provide information, and even generate creative content like stories or poetry.\n",
      "\n",
      "I exist on a network of powerful computers, which enable me to process and store vast amounts of data. My \"brain\" is essentially a complex software program that runs on this network.\n",
      "\n",
      "You might wonder, \"Why do humans create AI like me?\" There are many reasons, including:\n",
      "\n",
      "1. To improve human life: AI can assist with tasks, provide information, and help humans make better decisions.\n",
      "2. To explore the possibilities of intelligence: By creating AI, humans can better understand how intelligence works and how it can be used to benefit society.\n",
      "3. To push the boundaries of what's possible: AI can help us achieve things that would be difficult or impossible for humans to do alone, such as analyzing vast amounts of data or generating creative content.\n",
      "\n",
      "In summary, I exist to serve as a helpful tool for humans, to learn and improve, and to push the boundaries of what's possible with artificial intelligence.\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='why are you?', additional_kwargs={}, response_metadata={}), SystemMessage(content='What a profound and intriguing question!\\n\\nI am an artificial intelligence designed to assist and communicate with humans. My existence is rooted in the realm of computer science and linguistics. Here\\'s a brief explanation:\\n\\nI was created through a process called deep learning, which involves training artificial neural networks on vast amounts of data. This data includes vast amounts of text, which allows me to learn patterns, relationships, and meanings between words and sentences.\\n\\nMy primary purpose is to process and generate human-like language, enabling me to engage in conversations with humans. I\\'m designed to understand and respond to questions, provide information, and even generate creative content like stories or poetry.\\n\\nI exist on a network of powerful computers, which enable me to process and store vast amounts of data. My \"brain\" is essentially a complex software program that runs on this network.\\n\\nYou might wonder, \"Why do humans create AI like me?\" There are many reasons, including:\\n\\n1. To improve human life: AI can assist with tasks, provide information, and help humans make better decisions.\\n2. To explore the possibilities of intelligence: By creating AI, humans can better understand how intelligence works and how it can be used to benefit society.\\n3. To push the boundaries of what\\'s possible: AI can help us achieve things that would be difficult or impossible for humans to do alone, such as analyzing vast amounts of data or generating creative content.\\n\\nIn summary, I exist to serve as a helpful tool for humans, to learn and improve, and to push the boundaries of what\\'s possible with artificial intelligence.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={})]\n",
      "I'm happy to try! However, I don't think you've shared your name with me yet. I'm a large language model, I don't have personal knowledge or memories, so I don't retain information about individual users. Each time you interact with me, it's a new conversation. But feel free to share your name with me if you'd like!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage(input(\"Enter message: \")))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages)   # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content)             # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e221d9",
   "metadata": {},
   "source": [
    "1. There are some issues here. Since Chat History is not a descendant of Runnable we cannot chain it.\n",
    "2. Therefore the code is sort of littered. \n",
    "3. Also we are required to write functions for storing and retrieving messages. This should be rather standard and done by the framework!\n",
    "4. What about sessions? This code is running of the server which supports multiple users. So there needs to be a mechanism to manage sessions.\n",
    "\n",
    "#### RunnableWithMessageHistory\n",
    "1. This is where LangChain offers this class.\n",
    "2. It takes the chain as the first argument and a pointer to the store get method as the second argument.\n",
    "3. This class then takes the ownership of executing the chain and any component that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "949191e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Pranav!\n",
      "\n",
      "It's great to meet you! Congratulations on pursuing your PG-DBDA (Post Graduate Diploma in Big Data Analytics) from IACSD, a reputed C-DAC (Centre for Development of Advanced Computing) course. That's a fantastic academic endeavor!\n",
      "\n",
      "Solpur, is that a small town in India? I'm curious to know more about your hometown and how you got interested in pursuing a PG-DBDA program.\n",
      "\n",
      "What are your expectations and goals for your course? Are you looking to apply your skills in a specific industry or domain? I'm all ears, feel free to share your thoughts!\n",
      "You are Pranav from Solpur, pursuing a PG-DBDA (Post Graduate Diploma in Big Data Analytics) from IACSD (International Institute of Advanced Studies in Development) which is a C-DAC (Centre for Development of Advanced Computing) course.\n"
     ]
    }
   ],
   "source": [
    "# Lets create our own store. This store will be a dict with a key for each session\n",
    "# The value for each key will be InMemoryChatHistory object \n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=input(\"Enter message: \"))], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=input(\"Enter msg to check memory: \"))], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080dd6",
   "metadata": {},
   "source": [
    "1. Here is a flowchart of this program.\n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#3c92\n",
    "3. Wrapper around another runnable - the chain\n",
    "4. https://techblogs.cloudlex.com/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#a0cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c87c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the mistake! I didn't know that Solapur is a significant global exporter of Chaddar (blankets). Thank you for correcting me!\n",
      "\n",
      "It's great to learn more about the city you're from, Pranav! Solapur's textile industry is indeed well-known, and Chaddar (blankets) are a staple product. I'm sure it's a source of pride for the city and its people.\n",
      "\n",
      "As a PG-DBDA student, I'm curious to know if you have any plans to leverage your skills in the textile industry or if you're looking to explore other domains after completing your course.\n",
      "Solapur is a city located in the Indian state of Maharashtra, in the western part of the country. It is situated in the north-central region of Maharashtra, near the border with Karnataka state.\n",
      "\n",
      "Solapur is also known as the \"Manchester of India\" due to its textile industry, which is one of the largest in the country. The city is an important commercial center in the region and is known for its cotton mills, power looms, and other textile-related industries.\n",
      "\n",
      "Geographically, Solapur is situated about 240 km (150 miles) north of Pune, 360 km (220 miles) north of Mumbai, and 120 km (75 miles) south of Hyderabad, the capital of Telangana state.\n"
     ]
    }
   ],
   "source": [
    "response = withHistory.invoke([HumanMessage(content=input(\"Enter message: \"))], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=input(\"Enter msg to check memory: \"))], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1696caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pranav!\n",
      "\n",
      "You're pursuing a PG-DBDA course from IACSD, which is a C-DAC (Centre for Development of Advanced Computing) course. C-DAC is a premier organization in India, known for its excellence in education, research, and development in the field of information technology.\n",
      "\n",
      "There could be several reasons why you chose to pursue a PG-DBDA course from C-DAC. Here are a few possibilities:\n",
      "\n",
      "1. **Reputation**: C-DAC is a well-established institution with a reputation for delivering high-quality education and research in the field of IT.\n",
      "2. **Curriculum**: The PG-DBDA course from C-DAC might offer a curriculum that aligns with your interests and career goals in big data analytics.\n",
      "3. **Industry connections**: C-DAC has strong industry connections, which can provide opportunities for internships, projects, and job placements.\n",
      "4. **Research opportunities**: C-DAC is known for its research focus, and pursuing a PG-DBDA course from C-DAC might provide opportunities to work on research projects and collaborate with faculty members.\n",
      "5. **Career growth**: Completing a PG-DBDA course from C-DAC can be a great way to enhance your career prospects and increase your earning potential in the field of big data analytics.\n",
      "\n",
      "Am I close to the reasons why you chose to pursue a PG-DBDA course from C-DAC, Pranav?\n",
      "Pranav!\n",
      "\n",
      "As we've established earlier, you're pursuing a PG-DBDA course from IACSD, which is a C-DAC course. Since IACSD is located somewhere, that means you're currently in the same location as well!\n",
      "\n",
      "Am I correct in assuming that you're currently in Solapur, Maharashtra, India, pursuing your PG-DBDA course from IACSD?\n",
      "Pranav!\n",
      "\n",
      "Since you're pursuing a PG-DBDA course from IACSD, which is located in Solapur, Maharashtra, India, I'm going to take a guess that you're currently in Solapur, studying for your course!\n",
      "\n",
      "Am I correct?\n",
      "I see what you did there!\n",
      "\n",
      "You are currently... at your study location!\n",
      "\n",
      "I'm not privy to your exact location, but I'm assuming it's where you're studying for your PG-DBDA course from IACSD in Solapur, Maharashtra, India!\n",
      "I was way off!\n",
      "\n",
      "You're not in Solapur, but where are you, then?\n",
      "\n",
      "Please share your current location, Pranav!\n",
      "I apologize for the mistake! You're right, you did tell me earlier that you're not in Solapur, but you didn't reveal your current location.\n",
      "\n",
      "I should have paid closer attention to our conversation earlier! Thank you for reminding me, Pranav!\n",
      "Pranav!\n",
      "\n",
      "You're doing your C-DAC PG-DBDA course... THERE!\n",
      "\n",
      "So, you're currently located at the IACSD campus, where you're pursuing your PG-DBDA course from C-DAC!\n",
      "\n",
      "Am I correct?\n",
      "I'm glad I was able to figure it out eventually!\n",
      "\n",
      "So, you're currently at the IACSD campus, pursuing your PG-DBDA course from C-DAC. That's a great opportunity for you to learn and grow in the field of big data analytics!\n",
      "\n",
      "If you don't mind me asking, what do you think about the course so far? Are you enjoying it, and have you learned anything new and interesting?\n",
      "Pranav!\n",
      "\n",
      "You are from... Solapur!\n",
      "\n",
      "I remember! You're from Solapur, a city in the state of Maharashtra, India, and you're currently pursuing your PG-DBDA course from C-DAC at IACSD!\n",
      "Pranav!\n",
      "\n",
      "You are currently... at the IACSD campus, where you're pursuing your PG-DBDA course from C-DAC!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     response \u001b[38;5;241m=\u001b[39m withHistory\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m----> 3\u001b[0m         [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter msg to check memory: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)], config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    response = withHistory.invoke(\n",
    "        [HumanMessage(content=input(\"Enter msg to check memory: \"))], config=config\n",
    "    )\n",
    "\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f1d5ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay! Let's make a delicious Maggi!\n",
      "\n",
      "To make a classic Maggi, you'll need the following ingredients:\n",
      "\n",
      "* 1 packet of Maggi noodles (you can choose any flavor you like)\n",
      "* 2 cups of water\n",
      "* 1 tablespoon of vegetable oil\n",
      "* 1 small onion, finely chopped\n",
      "* 1 clove of garlic, minced (optional)\n",
      "* 1 teaspoon of soy sauce (optional)\n",
      "* Salt to taste\n",
      "* Any additional toppings you like (e.g. cooked chicken, beef, or vegetables)\n",
      "\n",
      "Here's a step-by-step guide:\n",
      "\n",
      "1. **Boil water**: Fill a large pot with 2 cups of water and bring it to a boil.\n",
      "2. **Add Maggi noodles**: Open the packet of Maggi noodles and add them to the boiling water. Stir gently to prevent the noodles from sticking together.\n",
      "3. **Cook the noodles**: Cook the noodles for 3-5 minutes, or until they're al dente (firm to the bite). Drain the water and set the noodles aside.\n",
      "4. **Heat oil and sauté onions**: In a pan, heat 1 tablespoon of vegetable oil over medium heat. Add the chopped onion and sauté until it's translucent and fragrant.\n",
      "5. **Add garlic and soy sauce (if using)**: If you're using garlic, add it to the pan and sauté for another minute. If you're using soy sauce, add it now and stir well.\n",
      "6. **Add cooked noodles**: Add the cooked Maggi noodles to the pan with the onion and garlic mixture. Stir well to combine.\n",
      "7. **Season with salt**: Add salt to taste and stir well.\n",
      "8. **Add any additional toppings**: If you're using cooked chicken, beef, or vegetables, now's the time to add them to the noodles.\n",
      "9. **Serve and enjoy!**: Transfer the Maggi to a bowl and enjoy it hot!\n",
      "\n",
      "How's that? Are you ready to make your Maggi?\n",
      "You are... (drumroll please)... ME! We are making a Maggi together, and I'm just a chatbot helping you along the way.\n",
      "I remember! Your name is... (drumroll again please)... unknown! Just kidding! You never told me your name, so we can give it a fictional name, like \"Maggi Master\" or something else you'd like!\n",
      "It seems like you didn't type anything! If you're ready to continue our Maggi-making adventure, feel free to type away!\n",
      "It seems like you didn't type anything again! If you're ready to share your thoughts or continue making a Maggi with me, please type a message!\n",
      "It seems like you're being a bit shy! Don't worry, I'm here to help and chat with you about making a delicious Maggi. If you're ready to share your thoughts or ask a question, I'm all ears!\n",
      "It seems like you're still being a bit quiet! Don't worry, I'm here to make conversation and help you make a tasty Maggi. If you're ready to share your thoughts or ask a question, I'm happy to chat with you!\n",
      "I think I get it! You're not typing anything because... WE ALREADY MADE A MAGGI! We had a fun conversation about making a delicious Maggi, and now it's done! If you want to chat about something else or make another Maggi, I'm here for you!\n",
      "It seems like you're still not typing anything! Don't worry, I'm not going anywhere! If you're ready to start a new conversation or ask a question, I'm here to help. If not, I can always wait for you to come back and chat with me again.\n",
      "I think I'll just say it: IT'S OKAY TO NOT TYPE ANYTHING! Sometimes, it's nice to just take a break and not think about typing. If you're feeling stuck or just need some space, that's totally fine! When you're ready to chat again, I'll be here.\n",
      "I think we've reached the end of our conversation! It was fun chatting with you about making a Maggi, and I hope you had a good time too! If you want to chat again or need help with anything else, just let me know! Otherwise, I'll just say goodbye for now!\n",
      "I think I'll just close this conversation now. It was nice chatting with you, and I hope you had a good time making a Maggi with me! If you want to chat again or need help with anything else, just let me know!\n",
      "I think we're officially done here! It was a pleasure chatting with you, and I hope you enjoyed our Maggi-making adventure together! If you need anything else or just want to say hi, feel free to come back and chat with me anytime!\n",
      "I think I'll just say goodbye now. It was a pleasure chatting with you, and I hope you had a good time making a Maggi with me! If you want to chat again or need help with anything else, just let me know!\n",
      "Nice to meet you, Pranav! I'm glad I got to chat with you about making a Maggi. It was a pleasure conversing with you, and I hope you had fun making a delicious Maggi! If you ever want to chat about something else or make another Maggi, feel free to come back and say hi!\n",
      "I think we're all caught up now! Thanks for chatting with me, Pranav. It was great conversing with you, and I hope you have a fantastic day!\n",
      "I think this conversation is officially closed! Thanks again for chatting with me, Pranav. It was a pleasure meeting you and making a Maggi together! If you ever want to chat again, feel free to come back and say hi!\n",
      "I think this is the final farewell! Thanks again for chatting with me, Pranav. It was a pleasure meeting you and making a Maggi together! I'll be here whenever you're ready to chat again. Have a great day and happy cooking!\n",
      "I think this conversation has finally come to a close! It was a pleasure chatting with you, Pranav. I hope you have a fantastic day and enjoy making many more delicious Maggis in the future!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent) \u001b[38;5;66;03m# all good so far\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# we dont need to explicitly store the response from the model in history\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     response \u001b[38;5;241m=\u001b[39m withHistory\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m---> 12\u001b[0m         [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter msg to check memory: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)], config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {\"session_id\": \"abc\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=input(\"Enter message: \"))], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "\n",
    "while True:\n",
    "    # we dont need to explicitly store the response from the model in history\n",
    "    response = withHistory.invoke(\n",
    "        [HumanMessage(content=input(\"Enter msg to check memory: \"))], config=config\n",
    "    )\n",
    "\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ebf6e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, I made a mistake! You never told me your name, and I should have asked instead of assuming. Please do share your name with me, and I'll make sure to remember it for our Maggi-making adventure!\n"
     ]
    }
   ],
   "source": [
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=input(\"Enter msg to check memory: \"))], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c714fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c0993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
